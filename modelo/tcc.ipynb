{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/dados_agregados.csv' does not exist",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e832ef27a212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/dados_agregados.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data/dados_agregados.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as pkl\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "#Configuração\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "cols_scouts_def = ['CA','CV','DD','DP','FC','GC','GS','RB','SG'] # alphabetical order\n",
    "cols_scouts_atk = ['A','FD','FF','FS','FT','G','I','PE','PP'] # alphabetical order\n",
    "cols_scouts = cols_scouts_def + cols_scouts_atk\n",
    "\n",
    "scouts_weights = np.array([-2.0, -5.0, 3.0, 7.0, -0.5, -6.0, -2.0, 1.7, 5.0, 5.0, 1.0, 0.7, 0.5, 3.5, 8.0, -0.5, -0.3, -3.5])\n",
    "\n",
    "ROUND_TO_PREDICT = 38\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/dados_agregados.csv')\n",
    "print(df.shape)\n",
    "df.head(10)\n",
    "\n",
    "#Processo de limpeza dos dados\n",
    "\n",
    "print(\"Dimensões originais dos dados: \", df.shape)\n",
    "\n",
    "# remove todas as linhas cujo scouts são NANs\n",
    "df_clean = df.dropna(how='all', subset=cols_scouts)\n",
    "print('qtde. de jogadores com scouts: ', df_clean.shape[0])\n",
    "\n",
    "# remove todas as linhas com rodada == 0\n",
    "df_clean = df_clean[df_clean['Rodada'] > 0]\n",
    "print(\"qtde. de linhas após eliminação da rodada 0: \", df_clean.shape[0])\n",
    "\n",
    "# remove técnicos e jogadores sem posição\n",
    "df_clean = df_clean[(df_clean['Posicao'] != \"tec\") & (~df_clean['Posicao'].isnull())]\n",
    "print(\"qtde. de linhas com posições válidas: \", df_clean.shape[0])\n",
    "\n",
    "# remove todos os jogadores que não participaram de alguma rodada\n",
    "df_clean = df_clean[(df_clean['Participou'] == True) | (df_clean['PrecoVariacao'] != 0)]\n",
    "print(\"qtde. de linhas com jogadores que participaram de alguma rodada: \", df_clean.shape[0])\n",
    "\n",
    "# altera os Status = NAN para 'Provável'\n",
    "df_clean.loc[df_clean.Status.isnull(), 'Status'] = 'Provável'\n",
    "\n",
    "# atualiza nomes dos jogadores sem ids e remove jogadores sem nome\n",
    "df_ids =  df.groupby('AtletaID')['Apelido'].unique()\n",
    "dict_ids = dict(zip(df_ids.index, [str(v[-1]) for v in df_ids.values]))\n",
    "dict_ids = {k:v for k,v in dict_ids.items() if v != 'nan'}\n",
    "df_clean['Apelido'] = df_clean['AtletaID'].map(dict_ids)\n",
    "df_clean = df_clean[~df_clean['Apelido'].isnull()]\n",
    "print(\"qtde. de jogadores com nome: \", df_clean.shape[0])\n",
    "\n",
    "# preenche NANs restantes com zeros (verificado antes!)\n",
    "df_clean.fillna(value=0, inplace=True)\n",
    "\n",
    "print(\"Dimensão dos dados após as limpezas: \", df_clean.shape)\n",
    "df_clean.head(10)\n",
    "\n",
    "\n",
    "#Atualização dos times para jogadores\n",
    "df_teams = pd.read_csv('./data/times_ids.csv')\n",
    "df_teams = df_teams.dropna()\n",
    "print(df_teams.shape)\n",
    "df_teams.head()\n",
    "\n",
    "\n",
    "# do not run this cell twice!\n",
    "dict_teams_id = dict(zip(df_teams['id'], df_teams['nome.cartola']))\n",
    "dict_teams_id.update(dict(zip(df_teams['cod.older'], df_teams['nome.cartola'])))\n",
    "\n",
    "df_clean['ClubeID'] = df_clean['ClubeID'].astype(np.int).map(dict_teams_id)\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "print(df_clean.shape)\n",
    "df_clean.head()\n",
    "\n",
    "#Atualização dos scouts cumulativos referentes ao ano de 2015\n",
    "\n",
    "def get_scouts_for_round(df, round_):\n",
    "    suffixes = ('_curr', '_prev')\n",
    "    cols_current = [col + suffixes[0] for col in cols_scouts]\n",
    "    cols_prev = [col + suffixes[1] for col in cols_scouts]\n",
    "\n",
    "    df_round = df[df['Rodada'] == round_]\n",
    "    if round_ == 1: return df_round\n",
    "\n",
    "    df_round_prev = df[df['Rodada'] < round_].groupby('AtletaID', as_index=False)[cols_scouts].max()\n",
    "    df_players = df_round.merge(df_round_prev, how='left', on=['AtletaID'], suffixes=suffixes)\n",
    "\n",
    "    # if is the first round of a player, the scouts of previous rounds will be NaNs. Thus, set them to zero\n",
    "    df_players.fillna(value=0, inplace=True)\n",
    "\n",
    "    # compute the scouts\n",
    "    df_players[cols_current] = df_players[cols_current].values - df_players[cols_prev].values\n",
    "\n",
    "    # update the columns\n",
    "    df_players.drop(labels=cols_prev, axis=1, inplace=True)\n",
    "    df_players = df_players.rename(columns=dict(zip(cols_current, cols_scouts)))\n",
    "    df_players.SG = df_players.SG.clip_lower(0)\n",
    "\n",
    "    return df_players\n",
    "\n",
    "\n",
    "df_scouts = df_clean[df_clean['ano'] != 2015]\n",
    "df_scouts_2015 = df_clean[df_clean['ano'] == 2015]\n",
    "\n",
    "n_rounds = df_scouts_2015['Rodada'].max()\n",
    "\n",
    "if np.isnan(n_rounds):\n",
    "    df_scouts = df_clean\n",
    "else:\n",
    "    for i in range(1, n_rounds + 1):\n",
    "        df_round = get_scouts_for_round(df_scouts_2015, i)\n",
    "        print(\"Dimensões da rodada #{0}: {1}\".format(i, df_round.shape))\n",
    "        df_scouts = df_scouts.append(df_round, ignore_index=True, sort=True)\n",
    "\n",
    "print(df_scouts.shape)\n",
    "df_scouts.head()\n",
    "\n",
    "# Verificar se a coluna de pontuação dos jogadores condiz com o scout\n",
    "\n",
    "def check_scouts(row):\n",
    "    return np.sum(scouts_weights*row[cols_scouts])\n",
    "\n",
    "players_points = df_scouts.apply(check_scouts, axis=1)\n",
    "errors = np.where(~np.isclose(df_scouts['Pontos'].values, players_points))[0]\n",
    "print(\"qtde. de jogadores com pontuação diferente dos scouts: \", errors.shape)\n",
    "df_scouts.iloc[errors, :].tail(10)\n",
    "\n",
    "# remove such players with wrong pontuation (DO NOT RUN TWICE!)\n",
    "df_scouts.reset_index(drop=True, inplace=True)\n",
    "df_scouts.drop(df.index[errors], inplace=True)\n",
    "print(df_scouts.shape)\n",
    "df_scouts.head()\n",
    "\n",
    "# Remover linhas duplicadas\n",
    "\n",
    "df_scouts.drop_duplicates(subset=['AtletaID', 'ano']+cols_scouts, keep='first', inplace=True)\n",
    "\n",
    "print(\"Dimensões dos dados após toda a limpeza de dados: \", df_scouts.shape)\n",
    "df_scouts.to_csv('./data/dados_agregados_limpos.csv', index=False)\n",
    "\n",
    "\n",
    "#Criação das amostras\n",
    "\n",
    "df_samples = pd.read_csv('./data/dados_agregados_limpos.csv')\n",
    "print(\"Dados para amostra\", df_samples.shape)\n",
    "df_samples.head()\n",
    "\n",
    "\n",
    "# seleciona somente as colunas de interesse para usar como atributos\n",
    "cols_of_interest = df_samples.columns.difference(['Apelido', 'Status', 'Participou', 'dia', 'mes']).values.tolist()\n",
    "\n",
    "# 'Rodada' e 'ano' serão usadas para criar amostras\n",
    "cols_info = ['Rodada', 'ano']\n",
    "\n",
    "df_samples = df_samples[cols_of_interest]\n",
    "df_samples.head()\n",
    "\n",
    "teams_full = pd.Series(df_samples['ClubeID'].unique()).sort_values().values\n",
    "\n",
    "def dict_positions(to_int = True):\n",
    "    dict_map = {'gol':1, 'zag':2, 'lat':3, 'mei':4, 'ata':5}\n",
    "    return  dict_map if to_int else dict(zip(dict_map.values(), dict_map.keys()))\n",
    "\n",
    "def dict_teams(to_int = True):\n",
    "    teams_map = {team:(index+1) for index, team in enumerate(teams_full)}\n",
    "    return teams_map if to_int else dict(zip(teams_map.values(), teams_map.keys()))\n",
    "\n",
    "print(dict_positions(), dict_teams(), sep='\\n')\n",
    "\n",
    "# mapeia \"casa\", \"atletas.clube_id\" and \"Posicao\" para números inteiros\n",
    "df_samples['ClubeID'] = df_samples['ClubeID'].map(dict_teams(to_int=True))\n",
    "df_samples['Posicao'] = df_samples['Posicao'].map(dict_positions(to_int=True))\n",
    "df_samples['variable'] = df_samples['variable'].map({'home.team':1, 'away.team':2})\n",
    "df_samples.head()\n",
    "\n",
    "\n",
    "df_samples.to_csv('./data/dados_agregados_amostras.csv', index=False)\n",
    "\n",
    "# Treinamento do modelo utilizando Redes Neurais Artificais, apenas com dados de 2017.\n",
    "\n",
    "df_samples = pd.read_csv('./data/dados_agregados_amostras.csv')\n",
    "df_samples = df_samples[df_samples.ano == 2017]\n",
    "print(df_samples.shape)\n",
    "df_samples.head()\n",
    "\n",
    "\n",
    "def create_samples(df, round_train, round_pred):\n",
    "    '''Create a Dataframe with players from round_train, but with 'Pontos' of round_pred'''\n",
    "    df_train = df[df['Rodada'] == round_train]\n",
    "    df_pred = df[df['Rodada'] == round_pred][['AtletaID', 'Pontos']]\n",
    "    df_merge = df_train.merge(df_pred, on='AtletaID', suffixes=['_train', '_pred'])\n",
    "\n",
    "    df_merge = df_merge.rename(columns={'Pontos_train': 'Pontos', 'Pontos_pred': 'pred'})\n",
    "\n",
    "    return df_merge\n",
    "\n",
    "df_train = pd.DataFrame(data=[], columns=list(df_samples.columns) + ['pred'])\n",
    "n_rounds = df_samples['Rodada'].max()\n",
    "\n",
    "for round_train, round_pred in zip(range(1, n_rounds), range(2, n_rounds + 1)):\n",
    "    df_round = create_samples(df_samples, round_train, round_pred)\n",
    "    print('qtde. de jogadores que participaram na rodada {0:=2} (train) e na rodada {1:=2} (pred): {2:=4}'.format(\n",
    "        round_train, round_pred, df_round.shape[0]))\n",
    "    df_train = df_train.append(df_round, ignore_index=True)\n",
    "\n",
    "print(\"Dimensões dos dados de treinamento: \", df_train.shape)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "samples = df_train[df_train.columns.difference(['AtletaID', 'Rodada','pred'])].values.astype(np.float64)\n",
    "scores  = df_train['pred'].values\n",
    "print(samples.shape, scores.shape)\n",
    "\n",
    "steps = [('MinMax', MinMaxScaler()), ('NN', MLPRegressor(solver='adam', activation='identity', learning_rate_init=1e-2, momentum=0.9, max_iter=2000))]\n",
    "pipe = Pipeline(steps)\n",
    "params = dict(NN__hidden_layer_sizes=[(50,50,50,50), (50,100,50), (50,100,100,50)])\n",
    "\n",
    "reg = GridSearchCV(pipe, params, scoring='neg_mean_squared_error', n_jobs=-1, cv=5, verbose=10)\n",
    "reg.fit(samples, scores)\n",
    "print(reg.best_params_, reg.best_score_)\n",
    "\n",
    "scores_pred = reg.predict(samples)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(range(scores.shape[0]), scores, color='blue')\n",
    "plt.plot(range(scores_pred.shape[0]), scores_pred, color='red')\n",
    "\n",
    "pkl.dump(reg, open('./modelo/nn1.pkl', 'wb'), -1)\n",
    "\n",
    "# Predições - Carregar modelo treinado e dizer melhores jogadores para uma próxima rodada.\n",
    "\n",
    "df_test = pd.read_csv('./data/dados_agregados_limpos.csv')\n",
    "df_test = df_test[df_test.ano == 2017]\n",
    "reg = pkl.load(open('./modelo/nn1.pkl', 'rb'))\n",
    "\n",
    "\n",
    "def to_samples(df):\n",
    "    df_samples = df[cols_info+cols_of_interest].copy()\n",
    "    df_samples['ClubeID'] = df_samples['ClubeID'].map(dict_teams(to_int=True))\n",
    "    df_samples['Posicao'] = df_samples['Posicao'].map(dict_positions(to_int=True))\n",
    "    df_samples['variable'] = df_samples['variable'].map({'home.team':1, 'away.team':2})\n",
    "    df_samples.reset_index(drop=True, inplace=True)\n",
    "    return df_samples\n",
    "\n",
    "def predict_best_players(df_samples, reg, n_players=11):\n",
    "    samples = df_samples[df_samples.columns.difference(['AtletaID', 'Rodada', 'ano'])].values.astype(np.float64)\n",
    "\n",
    "    pred = reg.predict(samples)\n",
    "    best_indexes = pred.argsort()[-n_players:]\n",
    "    return df_samples.iloc[best_indexes]\n",
    "\n",
    "def predict_best_players_by_position(df_samples, reg, n_gol=5, n_zag=5, n_lat=5, n_mei=5, n_atk=5):\n",
    "    df_result = pd.DataFrame(columns=df_samples.columns)\n",
    "    for n_players, pos in zip([n_gol, n_zag, n_lat, n_mei, n_atk], range(1, 6)):\n",
    "        samples = df_samples[df_samples['Posicao'] == pos]\n",
    "        df_pos = predict_best_players(samples, reg, n_players)\n",
    "        df_result = df_result.append(df_pos)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "df_rodada = df_test[(df_test['Rodada'] == (ROUND_TO_PREDICT-1)) & (df_test['Status'] == \"Provável\")]\n",
    "df_samples = to_samples(df_rodada)\n",
    "print(df_samples.shape)\n",
    "\n",
    "# Predição de jogadores.\n",
    "df_players = predict_best_players(df_samples, reg, n_players=25)\n",
    "df_rodada.iloc[df_players.index][['Apelido', 'Posicao', 'ClubeID']].sort_values('Posicao')\n",
    "\n",
    "# Predição de jogadores por posição.\n",
    "df_players = predict_best_players_by_position(df_samples, reg, n_gol=5, n_zag=5, n_lat=5, n_mei=5, n_atk=5)\n",
    "df_rodada.iloc[df_players.index][['Apelido', 'Posicao', 'ClubeID']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
